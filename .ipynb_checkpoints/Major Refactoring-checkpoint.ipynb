{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import csv\n",
    "import heapq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# utils\n",
    "from utils import normalized\n",
    "from utils import preprocessing_nltk\n",
    "from utils import compute_tfidf\n",
    "from utils import cosine_similarity\n",
    "from utils import distance_function\n",
    "\n",
    "# globals\n",
    "base_dir = './data/'\n",
    "tsv_dir = base_dir + 'tsv/'\n",
    "dataset = base_dir + 'Airbnb_Texas_Rentals.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list of words contained in a document\n",
    "# it simply is the document content\n",
    "docid2words = {}\n",
    "\n",
    "# vocabulary\n",
    "word2id = {}\n",
    "\n",
    "# documents that contain a precise word\n",
    "word2docid = {}\n",
    "\n",
    "# a collection of city, coords and document id\n",
    "geo2coords = {}\n",
    "\n",
    "# document id related to the city name\n",
    "docid2geo = {}\n",
    "\n",
    "# processing the main .csv and creating a no. of .tsv\n",
    "with open(dataset, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    # extracting data from each line\n",
    "    for i, elems in enumerate(csvreader): # line\n",
    "    \n",
    "        # skip header\n",
    "        if i == 0: continue\n",
    "        \n",
    "        # file index\n",
    "        index = elems[0]\n",
    "        \n",
    "        # geo (coords and city)\n",
    "        coords = (elems[6], elems[7])\n",
    "        city = elems[3]\n",
    "        \n",
    "        # preprocessing content (words)\n",
    "        descr = preprocessing_nltk(elems[5])\n",
    "        title = preprocessing_nltk(elems[8])\n",
    "        \n",
    "        # discarding docs with no words...\n",
    "        if len(descr) == 0 and len(title) == 0: continue\n",
    "        \n",
    "        # we don't want to deal with no location (needed in step 4)\n",
    "        if all(isinstance(c, float) for c in coords): continue  \n",
    "        \n",
    "        # add city and its coords (no duplicates)\n",
    "        if not city in geo2coords.keys():\n",
    "            geo2coords[city] = coords       \n",
    "\n",
    "        # docid2geo\n",
    "        docid2geo[index] = city\n",
    "        \n",
    "        # docid2words\n",
    "        docid2words[index] = []\n",
    "        docid2words[index].extend(descr)\n",
    "        docid2words[index].extend(title)\n",
    "        \n",
    "        # working with words to fill dictionaries\n",
    "        for word in docid2words[index]:\n",
    "            \n",
    "            # word2id\n",
    "            if not word in word2id.keys(): \n",
    "                word2id[word] = len(word2id.keys())\n",
    "            \n",
    "            # word2docid\n",
    "            if not word in word2docid.keys():\n",
    "                word2docid[word] = set([index])\n",
    "            else:\n",
    "                word2docid[word].add(index)\n",
    "        \n",
    "        # produce tsv\n",
    "        # put .tsv files into 'tsv' folder (that already has to exist)\n",
    "        with open(tsv_dir + 'doc_' + index + '.tsv', 'w') as doc_out:\n",
    "            doc_out.write('\\t'.join(elems[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the inverse index with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse index\n",
    "word2docid_tfidf = {}\n",
    "\n",
    "# docid2words_tfidf\n",
    "docid2words_tfidf = {}\n",
    "\n",
    "# create inverse index with tfidf\n",
    "for w, docs in word2docid.items():\n",
    "    \n",
    "    # skip in case we already have a word in the vocabulary\n",
    "    if w in word2docid_tfidf.keys(): continue\n",
    "    \n",
    "    # empty list (of future tuples)\n",
    "    word2docid_tfidf[w] = []\n",
    "    \n",
    "    # for each document that contains w\n",
    "    for d in docs:\n",
    "        \n",
    "        # create an empty structure if it's the first match\n",
    "        if not d in docid2words_tfidf.keys():\n",
    "            docid2words_tfidf[d] = {}\n",
    "        \n",
    "        # get document words (all its words)\n",
    "        content = docid2words[d]\n",
    "        \n",
    "        # compute tfidf\n",
    "        tfidf = compute_tfidf(content.count(w), len(content), len(docid2words.keys()), len(docs))\n",
    "        \n",
    "        # fill the vector\n",
    "        word2docid_tfidf[w].append((d, tfidf))\n",
    "        docid2words_tfidf[d][w] = tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['privat', 'entranc', 'cozi', 'histor', 'privat', 'studio']\n",
      "{'privat': 0.18737070484602142, 'cozi': 0.13566021410161513, 'entranc': 0.23998598119244557, 'histor': 0.19322314834126833, 'studio': 0.2496669036660849}\n"
     ]
    }
   ],
   "source": [
    "# sample of content for doc no. 10\n",
    "print(docid2words['10'])\n",
    "print(docid2words_tfidf['10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing document vectors\n",
    "docid2vec = {}\n",
    "\n",
    "# todo: use numpy\n",
    "for d, doc_words in docid2words.items():\n",
    "    \n",
    "    # skip if we have it already\n",
    "    if d in docid2vec.keys(): continue\n",
    "    \n",
    "    # empty vector\n",
    "    docid2vec[d] = []\n",
    "    \n",
    "    for w in word2id.keys():\n",
    "        \n",
    "        # if that word is the selected doc\n",
    "        if w in doc_words:\n",
    "            \n",
    "            # todo: fix\n",
    "            # skip if missing\n",
    "            if not d in docid2words_tfidf.keys():\n",
    "                continue\n",
    "            if not w in docid2words_tfidf[d]:\n",
    "                docid2vec[d].append(0.0)\n",
    "                continue\n",
    "            docid2vec[d].append(docid2words_tfidf[d][w])\n",
    "            \n",
    "        # else, fill with zeros    \n",
    "        else: \n",
    "            docid2vec[d].append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10127\n",
      "10127\n"
     ]
    }
   ],
   "source": [
    "# length of a document vector\n",
    "# it's the same length of the vocabulary\n",
    "print(len(docid2vec['999']))\n",
    "print(len(word2id.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a luxurious apartment downtown with pool quiet\n",
      "['luxuri', 'apart', 'downtown', 'pool', 'quiet']\n"
     ]
    }
   ],
   "source": [
    "# ask user\n",
    "query = input()\n",
    "query = preprocessing_nltk(query)\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build user input query vector\n",
    "uiq_vec = []\n",
    "\n",
    "# for each word in vocabulary\n",
    "for w in word2docid.keys():\n",
    "    \n",
    "    # default is 0.0\n",
    "    to_push = 0.0\n",
    "    \n",
    "    # if word is contained in user's query\n",
    "    # we need to push tfidf value for that word\n",
    "    if w in query:\n",
    "        to_push = compute_tfidf(query.count(w), len(query), len(docid2words.keys()), len(word2docid[w]))\n",
    "    \n",
    "    # add value in position \n",
    "    uiq_vec.append(to_push)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['13864', '637']\n"
     ]
    }
   ],
   "source": [
    "# here we are taking a list of documents numbers for each word in user query and\n",
    "# intersect these lists to receive only the numbers of documents which contain all of these query words.\n",
    "sets = []\n",
    "\n",
    "for q in query:\n",
    "    sets.append(word2docid.get(q) or set())\n",
    "matching_docs = list(set.intersection(*sets))\n",
    "\n",
    "print(len(matching_docs))\n",
    "print(matching_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13864\n",
      "Austin\n",
      "Quiet relax atmosphere just 10 minutes from downtown. 1 bed room 1 bath with washer dryer included,queen bed everything all white, balcony, 17ft tall ceilings, garden tub separate from shower. Privacy and quite neighbors, own desires parking spot. Pets welcome, gym open anytime with lap pool, tv outside and bar b q pit.\n",
      "30.2589482442465\n",
      "-97.8628891451995\n",
      "\n",
      "637\n",
      "Austin\n",
      "Quiet relax atmosphere just 10 minutes from downtown. 1 bed room 1 bath with washer dryer included,queen bed everything all white, balcony, 17ft tall ceilings, garden tub separate from shower. Privacy and quite neighbors, own desires parking spot. Pets welcome, gym open anytime with lap pool, tv outside and bar b q pit.\n",
      "30.2589482442465\n",
      "-97.8628891451995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# todo: show matching_docs through pandas\n",
    "for md in matching_docs:\n",
    "    df = pd.read_csv(tsv_dir + 'doc_' + md + '.tsv', sep='\\t', usecols=[2, 4, 5, 6])\n",
    "    print(md)\n",
    "    print(df.columns[0])\n",
    "    print(df.columns[1])\n",
    "    print(df.columns[2])\n",
    "    print(df.columns[3] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_docs_cos = []\n",
    "\n",
    "# to test it all over the docs -> docid2vec.keys()\n",
    "for d in matching_docs:\n",
    "    if len(uiq_vec) == len(docid2vec[d]):\n",
    "        matching_docs_cos.append((cosine_similarity(uiq_vec, docid2vec[d]), d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.20318214484274322, '637'), (0.20318214484274322, '13864')]\n"
     ]
    }
   ],
   "source": [
    "# creating heap structure\n",
    "heapq.heapify(matching_docs_cos)\n",
    "\n",
    "# showing the top-k where k = 10\n",
    "topk_cos = heapq.nlargest(10, matching_docs_cos)\n",
    "print(topk_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.20318214484274322, '637')\n",
      "Austin\n",
      "Quiet relax atmosphere just 10 minutes from downtown. 1 bed room 1 bath with washer dryer included,queen bed everything all white, balcony, 17ft tall ceilings, garden tub separate from shower. Privacy and quite neighbors, own desires parking spot. Pets welcome, gym open anytime with lap pool, tv outside and bar b q pit.\n",
      "30.2589482442465\n",
      "-97.8628891451995\n",
      "\n",
      "(0.20318214484274322, '13864')\n",
      "Austin\n",
      "Quiet relax atmosphere just 10 minutes from downtown. 1 bed room 1 bath with washer dryer included,queen bed everything all white, balcony, 17ft tall ceilings, garden tub separate from shower. Privacy and quite neighbors, own desires parking spot. Pets welcome, gym open anytime with lap pool, tv outside and bar b q pit.\n",
      "30.2589482442465\n",
      "-97.8628891451995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# todo: show top-k through pandas\n",
    "for md in topk_cos:\n",
    "    df = pd.read_csv(tsv_dir + 'doc_' + md[1] + '.tsv', sep='\\t', usecols=[2, 4, 5, 6])\n",
    "    print(md)\n",
    "    print(df.columns[0])\n",
    "    print(df.columns[1])\n",
    "    print(df.columns[2])\n",
    "    print(df.columns[3] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austin\n"
     ]
    }
   ],
   "source": [
    "# todo: geo2coords has to be preprocessed\n",
    "\n",
    "# ask user's position to show him/her\n",
    "# the better place nearby\n",
    "u_pos = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coords\n",
    "if not u_pos in geo2coords.keys():\n",
    "    print('This city is not supported!')\n",
    "\n",
    "coords = geo2coords[u_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('30.3095220042058', '-97.731710471095')\n"
     ]
    }
   ],
   "source": [
    "print(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "\n",
    "# for each matching document\n",
    "for m in matching_docs:\n",
    "    \n",
    "    # get city and its coords\n",
    "    d_city = docid2geo[m]\n",
    "    d_coords = geo2coords[d_city]\n",
    "    \n",
    "    # computing distance between user's location\n",
    "    # and each document's one\n",
    "    dist = distance_function(coords, d_coords)\n",
    "    distances.append((dist, m, d_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, '13864', 'Austin'), (0.0, '637', 'Austin')]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(distances)\n",
    "print(len(matching_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "distance_km = [x[0] for x in distances]\n",
    "\n",
    "# we need to invert the index as there's an inverse correlation\n",
    "# between the way we deal with our index from 0 to 1 and the distance \n",
    "distance_km_norm = 1.0 - normalized(np.asarray(distance_km, dtype=np.float32))\n",
    "\n",
    "print(distance_km_norm)\n",
    "print(len(distance_km_norm[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge cosine_similarity and distance\n",
    "matching_docs_ni = []\n",
    "\n",
    "# combining cosine_similarity and the new index\n",
    "for i in range(len(matching_docs_cos)):\n",
    "    m_doc = matching_docs_cos[i]\n",
    "    m_doc_id = m_doc[1]\n",
    "    \n",
    "    # cosine_similarity and normalized distance\n",
    "    m_doc_cos = m_doc[0]\n",
    "    dist_i = distance_km_norm[0][i]\n",
    "    \n",
    "    # weighted mean value\n",
    "    # giving more weight to the distance...\n",
    "    # ((w1 * x1) + (w2 * x2)) / (w1 + w2) \n",
    "    mean_v = ((m_doc_cos * 0.2) + (dist_i * 0.8))\n",
    "    matching_docs_ni.append((mean_v, m_doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8406364289685487, '637'), (0.8406364289685487, '13864')]\n"
     ]
    }
   ],
   "source": [
    "# creating heap structure\n",
    "heapq.heapify(matching_docs_ni)\n",
    "\n",
    "# showing the top-k where k = 10\n",
    "topk = heapq.nlargest(10, matching_docs_ni)\n",
    "print(topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8406364289685487, '637')\n",
      "Austin\n",
      "Quiet relax atmosphere just 10 minutes from downtown. 1 bed room 1 bath with washer dryer included,queen bed everything all white, balcony, 17ft tall ceilings, garden tub separate from shower. Privacy and quite neighbors, own desires parking spot. Pets welcome, gym open anytime with lap pool, tv outside and bar b q pit.\n",
      "30.2589482442465\n",
      "-97.8628891451995\n",
      "\n",
      "(0.8406364289685487, '13864')\n",
      "Austin\n",
      "Quiet relax atmosphere just 10 minutes from downtown. 1 bed room 1 bath with washer dryer included,queen bed everything all white, balcony, 17ft tall ceilings, garden tub separate from shower. Privacy and quite neighbors, own desires parking spot. Pets welcome, gym open anytime with lap pool, tv outside and bar b q pit.\n",
      "30.2589482442465\n",
      "-97.8628891451995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# todo: visualize data with pandas\n",
    "for md in topk:\n",
    "    df = pd.read_csv(tsv_dir + 'doc_' + md[1] + '.tsv', sep='\\t', usecols=[2, 4, 5, 6])\n",
    "    print(md)\n",
    "    print(df.columns[0])\n",
    "    print(df.columns[1])\n",
    "    print(df.columns[2])\n",
    "    print(df.columns[3] + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
